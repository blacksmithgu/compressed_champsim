/**
 * A "compression aware" version of Hawkeye which uses a size-aware OPTgen vector to naively
 * support compressed cache lines.
 */

#include "cache.h"
#include "champsim.h"
#include "instruction.h"

#include "size_aware_optgen.h"
#include "compression_tracker.h"
#include "counter.h"

#include <cstring>
#include <functional>
#include <map>
#include <memory>

// The maximum value a counter can take on in the predictor.
#ifndef COUNTER_MAX_VALUE
    #define COUNTER_MAX_VALUE 15
#endif

// The maximum value RRPV can reach in the cache.
#ifndef RRPV_MAX_VALUE
    #define RRPV_MAX_VALUE (MAX_COMPRESSIBILITY * LLC_WAY - 1)
#endif

// The predictor to use when making cache friendly/averse predictions.
#ifndef PREDICTOR
    #define PREDICTOR PCPredictor
#endif

// The score function and reducer function to compute which lines to evict. The *lowest* score line will be evicted.
#ifndef SCORE_FUNC
    #define SCORE_FUNC score_rrpv
#endif

#ifndef REDUCER_FUNC
    #define REDUCER_FUNC reducer_sum
#endif

// The cache model/generator to use; OPTgen by default.
#ifndef CACHEGEN
    #define CACHEGEN YACCgen<8192>
#endif

using namespace std;

/**
 * A structure containing all of the relevant information about a cache access.
 */
struct CacheAccess {
    // The cpu the access originated from.
    uint32_t cpu;

    // The specific set which this access went to.
    uint32_t set;

    // The specific memory address which was accessed.
    uint64_t full_address;

    // The address of the start of the cache line the memory address is a part of.
    uint64_t line_address;

    // The Optgen-specific time quanta that this access occurred at, measured in accesses to that Optgen structure.
    uint32_t optgen_time;

    // The PC which generated this access.
    uint64_t pc;

    // The compression factor of the cache line upon insertion.
    uint32_t compression_factor;

    // The size of the compressed cache line, in bytes.
    uint32_t compressed_size;

    // The prediction made for this cache access (false for miss, true for hit).
    bool prediction;

    CacheAccess(uint32_t cpu, uint32_t set, uint64_t full_address, uint64_t line_address,
            uint32_t optgen_time, uint64_t pc, uint32_t cf, uint32_t cs, bool prediction)
        : cpu(cpu), set(set), full_address(full_address), line_address(line_address),
        optgen_time(optgen_time), pc(pc), compression_factor(cf), compressed_size(cs), prediction(prediction) {}

    CacheAccess() {}
};

/**
 * A generic interface for a predictor.
 */
struct Predictor {
    virtual ~Predictor() {}

    /**
     * Trains the given cache access positively.
     */
    virtual void train(const CacheAccess& access) = 0;

    /**
     * Detrains the given cache access (i.e., trains it negatively).
     */
    virtual void detrain(const CacheAccess& access) = 0;

    /**
     * Returns true if the given access is predicted to be cache friendly, false if
     * it is cache averse.
     */
    virtual bool is_friendly(const CacheAccess& access) = 0;
};

struct PCPredictor : public Predictor {
    PCPredictor() : counters() { }
    virtual ~PCPredictor() { }

    /** Detrain a PC by reducing it's counter. */
    virtual void detrain(const CacheAccess& access) override {
        if(counters.find(access.pc) == counters.end())
            counters[access.pc] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[access.pc].decrement();
    }

    /** Train a PC by increasing it's counter. */
    virtual void train(const CacheAccess& access) override {
        if(counters.find(access.pc) == counters.end())
            counters[access.pc] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[access.pc].increment();
    }

    virtual bool is_friendly(const CacheAccess& access) override {
        if(counters.find(access.pc) == counters.end())
            return true;

        bool friendly = counters[access.pc].value() >= COUNTER_MAX_VALUE / 2;
        return friendly;
    }
private:
    // PC-indexed counters.
    map<uint64_t, Counter<COUNTER_MAX_VALUE>> counters;
};

// A map-friendly struct which contains both a PC and a compression value.
struct PCAndCompression {
    uint64_t pc;
    uint32_t compression;

    PCAndCompression(uint64_t pc, uint32_t comp) : pc(pc), compression(comp) {}

    bool operator==(const PCAndCompression& other) const {
        return pc == other.pc && compression == other.compression;
    }

    bool operator<(const PCAndCompression& other) const {
        return pc < other.pc || (pc == other.pc && compression < other.compression);
    }
};

// Predictor which predicts using both PC and compression.
struct PCAndCompressionPredictor : public Predictor {
    PCAndCompressionPredictor() : counters() { }
    virtual ~PCAndCompressionPredictor() { }

    /** Detrain a PC/compression by reducing it's counter. */
    virtual void detrain(const CacheAccess& access) override {
        PCAndCompression pc_and_comp(access.pc, access.compression_factor);
        if(counters.find(pc_and_comp) == counters.end())
            counters[pc_and_comp] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[pc_and_comp].decrement();
    }

    /** Train a PC by increasing it's counter. */
    virtual void train(const CacheAccess& access) override {
        PCAndCompression pc_and_comp(access.pc, access.compression_factor);
        if(counters.find(pc_and_comp) == counters.end())
            counters[pc_and_comp] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[pc_and_comp].increment();
    }

    virtual bool is_friendly(const CacheAccess& access) override {
        PCAndCompression pc_and_comp(access.pc, access.compression_factor);
        if(counters.find(pc_and_comp) == counters.end())
            return true;

        return counters[pc_and_comp].value() >= COUNTER_MAX_VALUE / 2;
    }
private:
    // PC AND compression-indexed counters.
    map<PCAndCompression, Counter<COUNTER_MAX_VALUE>> counters;
};

struct SizePredictor : public Predictor {
    SizePredictor() : counters() { }
    virtual ~SizePredictor() { }

    uint8_t bucket_for(uint32_t compressed_size) {
        return uint8_t(compressed_size / 8);
    }

    /** Detrain a compressed size by reducing it's counter. */
    virtual void detrain(const CacheAccess& access) override {
        uint8_t bucket = bucket_for(access.compressed_size);
        if(counters.find(bucket) == counters.end())
            counters[bucket] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[bucket].decrement();
    }

    /** Train a PC by increasing it's counter. */
    virtual void train(const CacheAccess& access) override {
        uint8_t bucket = bucket_for(access.compressed_size);
        if(counters.find(bucket) == counters.end())
            counters[bucket] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[bucket].increment();
    }

    virtual bool is_friendly(const CacheAccess& access) override {
        uint8_t bucket = bucket_for(access.compressed_size);
        if(counters.find(bucket) == counters.end())
            return true;

        return counters[bucket].value() >= COUNTER_MAX_VALUE / 2;
    }
private:
    // PC-indexed counters.
    map<uint64_t, Counter<COUNTER_MAX_VALUE>> counters;
};

// A map of address -> full cache accesses which we use in order to compute the optimal solution.
map<uint64_t, CacheAccess> outstanding_accesses;

// The predictor to use when making caching decisions.
std::shared_ptr<Predictor> predictor;

// A per-set collection of cachegen data structures, used to compute the optimal labels of specific cache accesses.
CACHEGEN optgens[LLC_SET];

// A local "timer" counting the number of accesses to each set/optgen.
uint32_t num_accesses[LLC_SET] = {0};

// The full required state for keeping track of RRPV; this is computed on a per-block level, and then the overall
// RRPV for an entire superblock is computed as the best RRPV of the sub-blocks.
uint32_t rrpv[LLC_SET][LLC_WAY][MAX_COMPRESSIBILITY] = {0};

// Contains metadata about all of the cache lines in the cache, indexed by set, way, and compressed index.
CacheAccess accesses[LLC_SET][LLC_WAY][MAX_COMPRESSIBILITY];

// Number of times YACCgen would report a hit/miss.
uint64_t cachegen_hits = 0, cachegen_misses = 0;

// Computes the "score" for a single entry within a superblock.
std::function<uint32_t(uint32_t rrpv, uint32_t cf, uint32_t cs)> score_func;

// Computes the score for a superblock, given the scores of each block within the superblock.
std::function<uint32_t(uint32_t* scores, uint32_t count, uint32_t cf)> reducer_func;

uint32_t score_rrpv(uint32_t rrpv, uint32_t cf, uint32_t cs) { return RRPV_MAX_VALUE - rrpv + 1; }
uint32_t score_mve(uint32_t rrpv, uint32_t cf, uint32_t cs) {
    return ((RRPV_MAX_VALUE - rrpv + 1) * MAX_COMPRESSIBILITY) / (MAX_COMPRESSIBILITY / cf);
}

uint32_t reducer_sum(uint32_t* scores, uint32_t count, uint32_t cf) {
    uint32_t sum = 0;
    for(uint32_t index = 0; index < count; index++) sum += scores[index];
    return sum;
}

uint32_t reducer_average(uint32_t* scores, uint32_t count, uint32_t cf) {
    return reducer_sum(scores, count, cf) / count;
}

// Tracks compressibility of lines coming into the cache.
CompressionTracker compression_tracker;

/** Ages the RRPV of all lines by 1 in a set, saturating at RRPV_MAX_VALUE. */
void rrpv_age_lines(uint32_t set_index, COMPRESSED_CACHE_BLOCK* set) {
    // First, see if the cache-friendly lines have already reached the saturation point.
    // If they have, there's nothing for us to do.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        for(uint32_t ci = 0; ci < set[way].compressionFactor; ci++) {
            if(!set[way].valid[ci]) continue;
            assert(rrpv[set_index][way][ci] <= RRPV_MAX_VALUE);

            if(rrpv[set_index][way][ci] == RRPV_MAX_VALUE - 1) return;
        }
    }

    // Otherwise, increment every cache-friendly line by 1.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        for(uint32_t ci = 0; ci < set[way].compressionFactor; ci++) {
            if(!set[way].valid[ci]) continue;

            if(rrpv[set_index][way][ci] < RRPV_MAX_VALUE - 1)
                rrpv[set_index][way][ci]++;
        }
    }
}

/** Called to initialize the LLC replacement policy state. */
void CACHE::llc_initialize_replacement() {
    // Initialize our optgen structures.
    for(int x = 0; x < LLC_SET; x++) optgens[x] = CACHEGEN(LLC_WAY);

    // Initialize the eviction strategy using the proper enviroment variables.
    score_func = SCORE_FUNC;
    reducer_func = REDUCER_FUNC;

    // Similarly initialize the predictor, using the compiler variable PREDICTOR.
    // For now, sub-optimally, we won't allow any arguments to be provided.
    predictor = std::make_shared<PREDICTOR>();
}

/**
 * Called when we need to find a victim to evict in a given set.
 * Notes: incoming_cf is the compression factor of the incoming cache line (1, 2, or 4 for now).
 * evicted_compressed_index is the index of the line within a compressed cache line to evict.
 */
uint32_t CACHE::llc_find_victim_cc(uint32_t cpu, uint64_t instr_id, uint32_t set, const COMPRESSED_CACHE_BLOCK *current_set,
        uint64_t ip, uint64_t full_addr, uint32_t type, uint64_t incoming_cf, uint64_t incoming_size,
        uint32_t& evicted_compressed_index) {
    // 1st Variant: Look for empty/invalid space in a superblock line.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        // Ignore lines w/ a different superblock or compression factor.
        if(current_set[way].sbTag != get_sb_tag(full_addr >> LOG2_BLOCK_SIZE)) continue;
        if(current_set[way].compressionFactor != incoming_cf) continue;

        for(uint32_t compression_index = 0; compression_index < current_set[way].compressionFactor; compression_index++) {
            if(current_set[way].valid[compression_index]) continue;

            // We've found an invalid line, return it.
            evicted_compressed_index = compression_index;
            return way;
        }
    }

    // 2nd Variant: Look for a plain invalid way.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        if(current_set[way].compressionFactor != 0) continue;

        // Otherwise, we found a line.
        evicted_compressed_index = MAX_COMPRESSIBILITY;
        return way;
    }

    // 3rd Variant: We now have to evict either another line of the same size (and same superblock), or an entire
    // superblock. We'll use MVE for this, computing the 'value' of each cache line and evicting the lowest value cache
    // line or superblock.
    uint32_t minimum_score = UINT_MAX;
    uint32_t victim_way = LLC_WAY, victim_ci = MAX_COMPRESSIBILITY;
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        // If this way is the same superblock as the incoming line, then consider each of it's line individually as
        // well.
        if(current_set[way].sbTag == get_sb_tag(full_addr >> LOG2_BLOCK_SIZE)
                && current_set[way].compressionFactor == incoming_cf) {
            for(uint32_t ci = 0; ci < current_set[way].compressionFactor; ci++) {
                assert(current_set[way].valid[ci]);

                const uint32_t score = score_func(rrpv[set][way][ci], current_set[way].compressionFactor,
                        accesses[set][way][ci].compressed_size);
                if(score < minimum_score) {
                    minimum_score = score;
                    victim_way = way;
                    victim_ci = ci;
                }
            }
        } else {
            // Compute the RRPV of the superblock according to the current strategy.
            uint32_t scores[MAX_COMPRESSIBILITY];
            uint32_t index = 0;
            for(uint32_t ci = 0; ci < current_set[way].compressionFactor; ci++) {
                if(current_set[way].valid[ci])
                    scores[index++] = score_func(rrpv[set][way][ci], current_set[way].compressionFactor,
                            accesses[set][way][ci].compressed_size);
            }

            assert(index > 0);

            const uint32_t score = reducer_func(scores, index, current_set[way].compressionFactor);
            if(score < minimum_score) {
                minimum_score = score;
                victim_way = way;
                victim_ci = MAX_COMPRESSIBILITY;
            }
        }
    }

    // Go through each line that will be evicted, and detrain it.
    if(victim_ci < MAX_COMPRESSIBILITY) {
        predictor->detrain(accesses[set][victim_way][victim_ci]);
    } else {
        for(uint32_t ci = 0; ci < current_set[victim_way].compressionFactor; ci++) {
            predictor->detrain(accesses[set][victim_way][ci]);
        }
    }

    evicted_compressed_index = victim_ci;
    return victim_way;
}

// TODO: simulator should generate empty implementations of these if mCOMPRESSED_CACHE is defined.
uint32_t CACHE::llc_find_victim(uint32_t cpu, uint64_t instr_id, uint32_t set, const BLOCK *current_set, uint64_t ip,
    uint64_t full_addr, uint32_t type) {
    std::cerr << "Normal find victim also called..." << std::endl;

    // An assert() is legitimate here instead of an exit(), because this should never happen.
    assert(0);
    return 0;
}

// Called on every cache hit and cache fill to update the replacement state.
void CACHE::llc_update_replacement_state_cc(uint32_t cpu, uint32_t set, uint32_t way, uint32_t compressed_index,
        uint64_t full_addr, uint64_t ip, uint64_t victim_addr, uint32_t type, uint32_t compression_factor,
        uint32_t compressed_size, uint8_t hit, uint64_t latency, uint64_t effective_latency) {
    // Writebacks just add semi-random noise to cache accesses, so ignore them.
    if (type == WRITEBACK) return;

    assert(set < LLC_SET && way < LLC_WAY);
    //printf("Index: %d, factor: %d\n", compressed_index, compression_factor);
    assert(compression_factor <= MAX_COMPRESSIBILITY && compressed_index < compression_factor);

    // Record compressibility of this line.
    compression_tracker.increment(compression_factor);

    // Compute the address of the corresponding cache line, using that to create the cache access and do all the
    // memory-related operations.
    uint64_t line_addr = full_addr & ~(BLOCK_SIZE - 1);

    // TODO: Move this nice printing stuff to a utility method somewhere for future debugging.
    /*
    if(set == 0) {
        printf("%u (hit: %s, way: %u, ci: %u, cf: %u, la: %lx): [", num_accesses[set], hit ? "true" : "false",
                way, compressed_index, compression_factor, line_addr);
        for(int way = 0; way < LLC_WAY; way++) {
            printf("[");
            for(int ci = 0; ci < (int)compressed_cache_block[set][way].compressionFactor; ci++) {
                if(ci != 0) printf(", ");
                if(compressed_cache_block[set][way].valid[ci]) printf("%u", rrpv[set][way][ci]);
                else printf("-");
            }
            printf("]");
        }
        printf("]\n");
    }
    */

    // If we've seen an access to this cache line before, then update Optgen and output this cache access.
    auto access_iter = outstanding_accesses.find(line_addr);
    if(access_iter != outstanding_accesses.end()) {
        CacheAccess& first_access = access_iter->second;

        assert(access_iter->first == first_access.line_address);
        assert(first_access.optgen_time <= num_accesses[set]);

        // Record the access in Optgen to get the hit/miss decision.
        uint64_t superblock = get_sb_tag(first_access.full_address >> LOG2_BLOCK_SIZE);
        bool decision = optgens[set].try_cache(first_access.optgen_time, num_accesses[set],
                superblock, first_access.compression_factor);

        // Train this PC up or down in our counters based on the decision.
        if(decision) predictor->train(first_access);
        else predictor->detrain(first_access);

        if(decision) cachegen_hits++; else cachegen_misses++;
    } else {
        // First time we've seen this access, compulsory miss.
        cachegen_misses++;
    }

    // Make a prediction based on the current counter values!
    // TODO: The access->prediction field is set after, very awkwardly...
    CacheAccess access = CacheAccess(cpu, set, full_addr, line_addr, num_accesses[set], ip,
            compression_factor, compressed_size, true);
    access.prediction = predictor->is_friendly(access);

    // Update the state of the newly inserted line, including setting up the RRPV and aging other lines in the cache.
    accesses[set][way][compressed_index] = access;

    // Compute the line RRPV appropriately (and potentially age other lines).
    if(access.prediction) {
        rrpv[set][way][compressed_index] = 0;
        if(!hit) rrpv_age_lines(set, this->compressed_cache_block[set]);
        rrpv[set][way][compressed_index] = 0;
    } else {
        rrpv[set][way][compressed_index] = RRPV_MAX_VALUE;
    }

    // Finally, update the access in the access map so we can observe future reuses.
    outstanding_accesses[line_addr] = access;
    num_accesses[set]++;
}

// called on every cache hit and cache fill
void CACHE::llc_update_replacement_state(uint32_t cpu, uint32_t set, uint32_t way, uint64_t full_addr, uint64_t ip,
    uint64_t victim_addr, uint32_t type, uint8_t hit, uint64_t latency, uint64_t effective_latency) {
    std::cout << "Normal Update replacement state called..." << std::endl;

    // Assert is valid here as this should never happen.
    assert(0);
}

void CACHE::llc_replacement_final_stats() {
    printf("\n");
    compression_tracker.print();

    printf("\n\n");
    printf("Cachegen Performance: %lu hits / %lu misses (%.2f)\n", cachegen_hits, cachegen_misses,
            double(cachegen_hits) / double(cachegen_hits + cachegen_misses) * 100.0);
}
