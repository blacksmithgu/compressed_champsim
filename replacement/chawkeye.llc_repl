/**
 * A "compression aware" version of Hawkeye which uses a size-aware OPTgen vector to naively
 * support compressed cache lines.
 */

#include "cache.h"
#include "champsim.h"
#include "instruction.h"

#include "size_aware_optgen.h"
#include "compression_tracker.h"
#include "counter.h"

#include <cstring>
#include <functional>
#include <map>
#include <memory>

// The maximum value a counter can take on in the predictor.
#define COUNTER_MAX_VALUE 15

// The maximum value RRPV can reach in the cache.
#define RRPV_MAX_VALUE (MAX_COMPRESSIBILITY * LLC_WAY - 1)

// The predictor to use when making cache friendly/averse predictions.
#ifndef PREDICTOR
    #define PREDICTOR PCPredictor
#endif

// The RRPV policy to use to compute an overall RRPV score for a cache way, given multiple RRPV scores for each subblock
// in a way.
#ifndef RRPV_STRATEGY
    #define RRPV_STRATEGY rrpv_average
#endif

// The cache model/generator to use; OPTgen by default.
#ifndef CACHEGEN
    #define CACHEGEN YACCgen<1024>
#endif

using namespace std;

/**
 * A structure containing all of the relevant information about a cache access.
 */
struct CacheAccess {
    // The cpu the access originated from.
    uint32_t cpu;

    // The specific set which this access went to.
    uint32_t set;

    // The specific memory address which was accessed.
    uint32_t full_address;

    // The address of the start of the cache line the memory address is a part of.
    uint32_t line_address;

    // The Optgen-specific time quanta that this access occurred at, measured in accesses to that Optgen structure.
    uint32_t optgen_time;

    // The PC which generated this access.
    uint32_t pc;

    // The compression factor of the cache line upon insertion.
    uint32_t compression_factor;

    // The prediction made for this cache access (false for miss, true for hit).
    bool prediction;

    CacheAccess(uint32_t cpu, uint32_t set, uint32_t full_address, uint32_t line_address,
            uint32_t optgen_time, uint32_t pc, uint32_t cf, bool prediction)
        : cpu(cpu), set(set), full_address(full_address), line_address(line_address),
        optgen_time(optgen_time), pc(pc), compression_factor(cf), prediction(prediction) {}

    CacheAccess() {}
};

/**
 * A generic interface for a predictor.
 */
struct Predictor {
    virtual ~Predictor() {}

    /**
     * Trains the given cache access positively.
     */
    virtual void train(const CacheAccess& access) = 0;

    /**
     * Detrains the given cache access (i.e., trains it negatively).
     */
    virtual void detrain(const CacheAccess& access) = 0;

    /**
     * Returns true if the given access is predicted to be cache friendly, false if
     * it is cache averse.
     */
    virtual bool is_friendly(const CacheAccess& access) = 0;
};

struct PCPredictor : public Predictor {
    PCPredictor() : counters() { }
    virtual ~PCPredictor() { }

    /** Detrain a PC by reducing it's counter. */
    virtual void detrain(const CacheAccess& access) override {
        if(counters.find(access.pc) == counters.end())
            counters[access.pc] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[access.pc].decrement();
    }

    /** Train a PC by increasing it's counter. */
    virtual void train(const CacheAccess& access) override {
        if(counters.find(access.pc) == counters.end())
            counters[access.pc] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[access.pc].increment();
    }

    virtual bool is_friendly(const CacheAccess& access) override {
        if(counters.find(access.pc) == counters.end())
            return (COUNTER_MAX_VALUE / 2);

        return counters[access.pc].value() >= COUNTER_MAX_VALUE / 2;
    }
private:
    // PC-indexed counters.
    map<uint64_t, Counter<COUNTER_MAX_VALUE>> counters;
};

// A map-friendly struct which contains both a PC and a compression value.
struct PCAndCompression { 
    uint64_t pc;
    uint32_t compression;

    PCAndCompression(uint64_t pc, uint32_t comp) : pc(pc), compression(comp) {}

    bool operator==(const PCAndCompression& other) const {
        return pc == other.pc && compression == other.compression;
    }

    bool operator<(const PCAndCompression& other) const {
        return pc < other.pc || (pc == other.pc && compression < other.compression);
    }
};

// Predictor which predicts using both PC and compression.
struct PCAndCompressionPredictor : public Predictor {
    PCAndCompressionPredictor() : counters() { }
    virtual ~PCAndCompressionPredictor() { }

    /** Detrain a PC/compression by reducing it's counter. */
    virtual void detrain(const CacheAccess& access) override {
        PCAndCompression pc_and_comp(access.pc, access.compression_factor);
        if(counters.find(pc_and_comp) == counters.end())
            counters[pc_and_comp] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[pc_and_comp].decrement();
    }

    /** Train a PC by increasing it's counter. */
    virtual void train(const CacheAccess& access) override {
        PCAndCompression pc_and_comp(access.pc, access.compression_factor);
        if(counters.find(pc_and_comp) == counters.end())
            counters[pc_and_comp] = Counter<COUNTER_MAX_VALUE>(COUNTER_MAX_VALUE / 2);

        counters[pc_and_comp].increment();
    }

    virtual bool is_friendly(const CacheAccess& access) override {
        PCAndCompression pc_and_comp(access.pc, access.compression_factor);
        if(counters.find(pc_and_comp) == counters.end())
            return (COUNTER_MAX_VALUE / 2);

        return counters[pc_and_comp].value() >= COUNTER_MAX_VALUE / 2;
    }
private:
    // PC AND compression-indexed counters.
    map<PCAndCompression, Counter<COUNTER_MAX_VALUE>> counters;
};

// A map of address -> full cache accesses which we use in order to compute the optimal solution.
map<uint64_t, CacheAccess> outstanding_accesses;

// The predictor to use when making caching decisions.
std::shared_ptr<Predictor> predictor;

// A per-set collection of cachegen data structures, used to compute the optimal labels of specific cache accesses.
CACHEGEN optgens[LLC_SET];

// A local "timer" counting the number of accesses to each set/optgen.
uint32_t num_accesses[LLC_SET] = {0};

// The full required state for keeping track of RRPV; this is computed on a per-block level, and then the overall
// RRPV for an entire superblock is computed as the best RRPV of the sub-blocks.
uint32_t rrpv[LLC_SET][LLC_WAY][MAX_COMPRESSIBILITY] = {0};

// Contains metadata about all of the cache lines in the cache, indexed by set, way, and compressed index.
CacheAccess accesses[LLC_SET][LLC_WAY][MAX_COMPRESSIBILITY];

// Given a compressed cache block for a way, and an array of the way RRPV values, returns the RRPV for that way.
// Initialized later, in CACHE::llc_initialize_replacement().
std::function<uint32_t(const COMPRESSED_CACHE_BLOCK& way, const uint32_t* way_rrpv)> rrpv_strategy;

// Tracks compressibility of lines coming into the cache.
CompressionTracker compression_tracker;

/** Computes the RRPV of a superblock to be the maximum of the subblock RRPV values. */
uint32_t rrpv_max(const COMPRESSED_CACHE_BLOCK& way, const uint32_t* way_rrpv) {
    uint32_t rrpv = 0;
    for(uint32_t index = 0; index < way.compressionFactor; index++) {
        if(!way.valid[index]) continue;

        rrpv = std::max(rrpv, way_rrpv[index]);
    }

    return rrpv;
}

/** Computes the RRPV of a superblock to be the minimum of the subblock RRPV values. */
uint32_t rrpv_min(const COMPRESSED_CACHE_BLOCK& way, const uint32_t* way_rrpv) {
    uint32_t rrpv = RRPV_MAX_VALUE;
    for(uint32_t index = 0; index < way.compressionFactor; index++) {
        if(!way.valid[index]) continue;

        rrpv = std::min(rrpv, way_rrpv[index]);
    }

    return rrpv;
}

/** Compute the RRPV as the average of the squared subblock RRPVs. */
uint32_t rrpv_square_sum(const COMPRESSED_CACHE_BLOCK& way, const uint32_t* way_rrpv) {
    uint32_t rrpv = 0, num_valid = 0;
    for(uint32_t index = 0; index < way.compressionFactor; index++) {
        if(!way.valid[index]) continue;

        rrpv += way_rrpv[index] * way_rrpv[index];
        num_valid++;
    }

    if(num_valid == 0) return 0;
    else return rrpv / num_valid;
}

/** Compute the RRPV by taking the average of the subblock RRPVs. */
uint32_t rrpv_average(const COMPRESSED_CACHE_BLOCK& way, const uint32_t* way_rrpv) {
    uint32_t rrpv = 0, num_valid = 0;
    for(uint32_t index = 0; index < way.compressionFactor; index++) {
        if(!way.valid[index]) continue;

        rrpv += way_rrpv[index];
        num_valid++;
    }

    if(num_valid == 0) return 0;
    else return rrpv / num_valid;
}

/**
 * Compute the RRPV by summing the existing RRPV entries, and rescaling the result by
 * (MAX_COMPRESSIBILITY / num_valid_entries).
 */
uint32_t rrpv_scaled_average(const COMPRESSED_CACHE_BLOCK& way, const uint32_t* way_rrpv) {
    uint32_t rrpv = 0, num_valid = 0;
    for(uint32_t index = 0; index < way.compressionFactor; index++) {
        if(!way.valid[index]) continue;

        rrpv += way_rrpv[index];
        num_valid++;
    }

    uint32_t average = rrpv_average(way, way_rrpv);
    while(num_valid < MAX_COMPRESSIBILITY) {
        rrpv += average;
        num_valid++;
    }

    return rrpv;
}

/** Ages the RRPV of all lines by 1 in a set, saturating at RRPV_MAX_VALUE. */
void rrpv_age_lines(uint32_t set_index, COMPRESSED_CACHE_BLOCK* set) {
    // First, see if the cache-friendly lines have already reached the saturation point.
    // If they have, there's nothing for us to do.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        for(uint32_t compression_index = 0; compression_index < set[way].compressionFactor; compression_index++) {
            if(!set[way].valid[compression_index]) continue;

            // Quit out if there is already a max-RRPV entry.
            if(accesses[set_index][way][compression_index].prediction
                    && rrpv[set_index][way][compression_index] == RRPV_MAX_VALUE) return;
        }
    }

    // Otherwise, increment every cache-friendly line by 1.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        for(uint32_t compression_index = 0; compression_index < set[way].compressionFactor; compression_index++) {
            if(!set[way].valid[compression_index]) continue;

            if(accesses[set_index][way][compression_index].prediction) rrpv[set_index][way][compression_index]++;
        }
    }
}

/** Called to initialize the LLC replacement policy state. */
void CACHE::llc_initialize_replacement() {
    // Initialize our optgen structures.
    for(int x = 0; x < LLC_SET; x++) optgens[x] = CACHEGEN(LLC_WAY);

    // Initialize the RRPV strategy, using the environment variable RRPV_STRATEGY.
    rrpv_strategy = RRPV_STRATEGY;

    // Similarly initialize the predictor, using the compiler variable PREDICTOR.
    // For now, sub-optimally, we won't allow any arguments to be provided.
    predictor = std::make_shared<PREDICTOR>();
}

/**
 * Called when we need to find a victim to evict in a given set.
 * Notes: incoming_cf is the compression factor of the incoming cache line (1, 2, or 4 for now).
 * evicted_compressed_index is the index of the line within a compressed cache line to evict.
 *
 * This assumes the YACC underlying cache architecture, which is a thankfully simple architecture. It only supports
 * homogenous cache lines (so same compressibility for all blocks in a way), and furthermore all blocks in a way must be
 * part of the same *superblock*; much like every 64 bytes makes a cache line in memory, every MAX_COMPRESSIBILITY cache
 * lines/"blocks" makes a superblock. Thus, this effectively means you can only store spatially contiguous, compressible
 * data in the same way in a YACC-based cache.
 *
 * With all these constraints in mind, we formulate a relatively "simple" cache replacement policy as follows, given a
 * new incoming line:
 * - Look for any ways which have the same superblock as the incoming line, and an empty space. Use that empty space.
 * - Look for a completely invalid way (no entries) and use that.
 * - Choose a superblock cacheline to evict, and evict all entries.
 *
 * The simplified cache heirarchy makes decisions easier, but 
 * - If CF = 1, then there's no compression anyway, so just choose a superblock cacheline to evict.
 * - IF CF = 2, it's tricky - we can potentially evict a line in a CF = 2 line with the same superblock, OR evict a
 *   different superblock to make space. We'll solve this by picking a superblock to evict as normal, and if the
 *   superblock happens to be the same superblock, then we only evict a single slot in it.
 * - If CF = 4, then all the lines of the superblock fit inside one line anyway.
 *
 * TODO: What if a good block gets stuck with a bad block in a 2x compressible line? The good block will currently
 * "protect" the bad block from eviction.
 */
uint32_t CACHE::llc_find_victim_cc(uint32_t cpu, uint64_t instr_id, uint32_t set, const COMPRESSED_CACHE_BLOCK *current_set,
        uint64_t ip, uint64_t full_addr, uint32_t type, uint64_t incoming_cf, uint64_t incoming_size,
        uint32_t& evicted_compressed_index) {
    // 1st Variant: Look for empty/invalid space in a superblock line.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        // Ignore lines w/ a different superblock.
        if(current_set[way].sbTag != get_sb_tag(full_addr >> LOG2_BLOCK_SIZE)) continue;

        // Ignore lines of a different compression factor.
        if(current_set[way].compressionFactor != incoming_cf) continue;

        for(uint32_t compression_index = 0; compression_index < current_set[way].compressionFactor; compression_index++) {
            // Ignore valid lines.
            if(current_set[way].valid[compression_index]) continue;

            // We've found an invalid line, return it.
            evicted_compressed_index = compression_index;
            return way;
        }
    }

    // 2nd Variant: Look for a plain invalid way.
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        // Valid ways have CF > 0.
        if(current_set[way].compressionFactor != 0) continue;

        // Otherwise, we found a line.
        evicted_compressed_index = MAX_COMPRESSIBILITY;
        return way;
    }

    // Tricky detraining #1: Check each cache line, and if we wouldn't cache it now, detrain it.
    // For size aware, we need to make sure to pass the size of the cache line as well, which is just
    // max size divided by compression amount.
    /*
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        for(uint32_t compression_index = 0; compression_index < current_set[way].compressionFactor; compression_index++) {
            if(!current_set[way].valid[compression_index]) continue;

            uint64_t cf = accesses[set][way][compression_index].compression_factor;
            uint64_t superblock = get_sb_tag(accesses[set][way][compression_index].full_address >> 6);
            uint64_t quanta = accesses[set][way][compression_index].optgen_time;
            if(!optgens[set].can_cache(quanta, num_accesses[set], superblock, cf))
                predictor->detrain(accesses[set][way][compression_index]);
        }
    }
    */

    // 3rd Variant: We now have to evict either another line of the same size (and same superblock), or an entire
    // superblock. We'll use RRIP for this.
    uint32_t max_rrpv = 0;
    uint32_t victim_way = LLC_WAY, victim_cf = MAX_COMPRESSIBILITY;
    for(uint32_t way = 0; way < LLC_WAY; way++) {
        // If this way is the same superblock as the incoming line, then consider each of it's line individually as
        // well.
        if(current_set[way].sbTag == get_sb_tag(full_addr >> LOG2_BLOCK_SIZE)
                && current_set[way].compressionFactor == incoming_cf) {
            for(uint32_t cf = 0; cf < current_set[way].compressionFactor; cf++) {
                if(rrpv[set][way][cf] > max_rrpv) {
                    max_rrpv = rrpv[set][way][cf];
                    victim_way = way;
                    victim_cf = cf;
                }
            }
        } else {
            // Compute the RRPV of the superblock according to the current strategy.
            uint32_t superblock_rrpv = rrpv_strategy(current_set[way], rrpv[set][way]);

            if(superblock_rrpv > max_rrpv) {
                max_rrpv = superblock_rrpv;
                victim_way = way;
                victim_cf = MAX_COMPRESSIBILITY;
            }
        }
    }

    // Go through each line that will be evicted; if it betrayed expectations, then detrain it.
    if(victim_cf < MAX_COMPRESSIBILITY) {
        if(accesses[set][victim_way][victim_cf].prediction)
            predictor->detrain(accesses[set][victim_way][victim_cf]);
    } else {
        for(uint32_t cf = 0; cf < current_set[victim_way].compressionFactor; cf++) {
            if(accesses[set][victim_way][cf].prediction)
                predictor->detrain(accesses[set][victim_way][cf]);
        }
    }

    evicted_compressed_index = victim_cf;
    return victim_way;
}

// TODO: simulator should generate empty implementations of these if COMPRESSED_CACHE is defined.
uint32_t CACHE::llc_find_victim(uint32_t cpu, uint64_t instr_id, uint32_t set, const BLOCK *current_set, uint64_t ip,
    uint64_t full_addr, uint32_t type) {
    std::cerr << "Normal find victim also called..." << std::endl;

    // An assert() is legitimate here instead of an exit(), because this should never happen.
    assert(0);
    return 0;
}

// Called on every cache hit and cache fill to update the replacement state.
void CACHE::llc_update_replacement_state_cc(uint32_t cpu, uint32_t set, uint32_t way, uint32_t compressed_index, uint64_t full_addr,
        uint64_t ip, uint64_t victim_addr, uint32_t type, uint32_t cf, uint32_t compressed_size, uint8_t hit,
        uint64_t latency, uint64_t effective_latency) {
    // Writeback hit usually does not update the replacement state, so ignore it.
    if (hit && (type == WRITEBACK))
        return;

    // Record compressibility of this line.
    compression_tracker.increment(cf);

    // Compute the address of the corresponding cache line, using that to create the cache access and do all the
    // memory-related operations.
    uint64_t line_addr = full_addr & ~(BLOCK_SIZE - 1);

    // If we've seen an access to this cache line before, then update Optgen and output this cache access.
    auto access_iter = outstanding_accesses.find(line_addr);
    if(access_iter != outstanding_accesses.end()) {
        CacheAccess& access = access_iter->second;

        // Record the access in Optgen to get the hit/miss decision.
        uint64_t superblock = get_sb_tag(access.full_address >> LOG2_BLOCK_SIZE);
        bool decision = optgens[set].try_cache(access.optgen_time, num_accesses[set], superblock, access.compression_factor);

        // Train this PC up or down in our counters based on the decision.
        if(decision) predictor->train(access);
        else predictor->detrain(access);
    }

    // Make a prediction based on the current counter values!
    // TODO: The access->prediction field is set after, very awkwardly...
    CacheAccess access = CacheAccess(cpu, set, full_addr, line_addr, num_accesses[set], ip, cf, true);
    access.prediction = predictor->is_friendly(access);

    // Update the state of the newly inserted line, including setting up the RRPV and aging other lines in the cache.
    accesses[set][way][compressed_index] = access;

    // TODO: We need to do something smarter than just setting RRPV to whatever the latest prediction is! We should take
    // the best of the blocks in a given way.
    if(access.prediction) {
        rrpv[set][way][cf] = 0;
        if(!hit) rrpv_age_lines(set, this->compressed_cache_block[set]);
        rrpv[set][way][cf] = 0;
    } else {
        rrpv[set][way][cf] = RRPV_MAX_VALUE;
    }

    // Finally, update the access in the access map so we can observe future reuses.
    outstanding_accesses[line_addr] = access;
    num_accesses[set]++;
}

// called on every cache hit and cache fill
void CACHE::llc_update_replacement_state(uint32_t cpu, uint32_t set, uint32_t way, uint64_t full_addr, uint64_t ip,
    uint64_t victim_addr, uint32_t type, uint8_t hit, uint64_t latency, uint64_t effective_latency) {
    std::cout << "Normal Update replacement state called..." << std::endl;

    // Assert is valid here as this should never happen.
    assert(0);
}

void CACHE::llc_replacement_final_stats() {
    compression_tracker.print();
}
